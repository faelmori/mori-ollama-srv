# ğŸš€ Open-WebUI + Ollama with Docker Compose

This project sets up an environment to run Open-WebUI and Ollama using `docker compose`.

## ğŸ“¦ Installation and Execution

1. **Clone this repository**:
   ```sh
   git clone https://github.com/faelmori/mori-ollama-srv.git
   cd mori-ollama-srv
   ```

2. **Run the installation script**:
   ```sh
   bash ./install.sh
   ```

3. **Access the Open WebUI**:
   Open your browser and go to [http://localhost:3000](http://localhost:3000).

## ğŸ³ Docker Compose Services

- **llm-ui**: Runs the Open-WebUI service.
- **llm-app**: Runs the Ollama service.

## ğŸ› ï¸ Configuration

- **Volumes**:
   - `llm-ui-vol`: Stores data for the Open-WebUI backend.
   - `llm-app-vol`: Stores data for the Ollama service.

- **Networks**:
   - `hub-ass-priv-net`: Private network for internal communication.
   - `hub-ass-pub-net`: Public network for external access.

## ğŸ“‚ Directory Structure

```
mori-ollama-srv/
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ install.sh
â”œâ”€â”€ secrets/
â”‚   â”œâ”€â”€ .llm.ui.dev.env
â”‚   â”œâ”€â”€ .llm.ui.prd.env
â”‚   â”œâ”€â”€ .llm.app.dev.env
â”‚   â”œâ”€â”€ .llm.app.prd.env
â””â”€â”€ README.md
```

## ğŸ“„ License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

## ğŸŒŸ Contributing

Contributions are welcome! Please open an issue or submit a pull request.

## ğŸ“§ Contact

For any inquiries, please contact [rafa-mori](https://github.com/rafa-mori).
